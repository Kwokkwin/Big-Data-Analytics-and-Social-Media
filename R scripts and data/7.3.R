# Topic Modelling ---------------------------------------------------------

# Load packages required for this part

library(vosonSML)
library(magrittr)
library(tidytext)
library(textclean)
library(qdapRegex)
library(tm)
library(topicmodels)
library(slam)
library(Rmpfr)
library(dplyr)
library(ggplot2)


# Set up Twitter authentication variables

my_app_name <- "7230ICT_Qin"
my_api_key <- "O2JTa6TiP4RqFTi5AhIjaEO5b"
my_api_secret <- "IIRS6pR8vakdvK7UE8SOCuNa2wW9NBAUdT3kanXYt3J43TCOgq"
my_access_token <- "1501187128038166532-Iwrrqjt1TDszF1MRES9cwNwSTvzN2u"
my_access_token_secret <-"ducOdck8wd3L6UAaKysiMEI3gj8DqFN8YaS7oYVuBZ4Nr"


# Authenticate to Twitter and collect data

twitter_data <- Authenticate("twitter",
                             appName = my_app_name,
                             apiKey = my_api_key,
                             apiSecret = my_api_secret,
                             accessToken = my_access_token,
                             accessTokenSecret = my_access_token_secret) %>%
    Collect(searchTerm = "#EdSheeran",
            searchType = "recent",
            numTweets = 10000,
            lang = "en",
            includeRetweets = TRUE,
            writeToFile = TRUE)


# Clean the tweet text

clean_text <- twitter_data$text  %>% 
    rm_twitter_url() %>% 
    replace_url() %>% 
    replace_hash() %>% 
    replace_tag() %>% 
    replace_internet_slang() %>% 
    replace_emoji() %>% 
    replace_emoticon() %>% 
    replace_non_ascii() %>% 
    replace_contraction() %>% 
    gsub("[[:punct:]]", " ", .) %>% 
    gsub("[[:digit:]]", " ", .) %>% 
    gsub("[[:cntrl:]]", " ", .) %>% 
    gsub("\\s+", " ", .) %>% 
    tolower()


# Convert clean tweet vector into a document corpus (collection of documents)

text_corpus <- VCorpus(VectorSource(clean_text))

text_corpus[[1]]$content
text_corpus[[5]]$content


# Remove stop words

text_corpus <- text_corpus %>%
    tm_map(removeWords, stopwords(kind = "SMART")) 

text_corpus[[1]]$content

text_corpus[[5]]$content


# Transform corpus into a Document Term Matrix and remove 0 entries

doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]


# Optional: Remove objects and run garbage collection for faster processing

save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc() 
load("doc_term_matrix.RData")


# Create LDA model with k topics

lda_model <- LDA(dtm, k = 6)


# Generate topic probabilities for each word
# 'beta' shows the probability that this word was generated by that topic

tweet_topics <- tidy(lda_model, matrix = "beta")


# Visualise the top 10 terms per topic

top_terms <- tweet_topics %>%
    group_by(topic) %>%
    slice_max(beta, n = 10) %>% 
    ungroup() %>%
    arrange(topic, -beta)

top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()

